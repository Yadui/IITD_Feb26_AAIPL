{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d23839-c0c6-47bf-9f53-84a879ab00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/.hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/workspace/.hf_cache/datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/.hf_cache/transformers\"\n",
    "\n",
    "os.makedirs(\"/workspace/.hf_cache\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b93a9e-44cf-4924-83c2-6f799c7bfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_path = \"/workspace/AAIPL/hf_models/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8\"\n",
    "\n",
    "dataset_path = \"synthetic_dataset.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6062c1a8-95ea-4e86-a2ef-e104a7116d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File length: 568001\n"
     ]
    }
   ],
   "source": [
    "with open(\"synthetic_dataset.json\", \"r\") as f:\n",
    "    raw = f.read()\n",
    "\n",
    "print(\"File length:\", len(raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0be6e62-e6a1-4ade-a1ef-2c3f38f09be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded objects: 312\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove trailing comma before final ]\n",
    "fixed = re.sub(r',\\s*\\]', ']', raw)\n",
    "\n",
    "# Try loading\n",
    "import json\n",
    "data = json.loads(fixed)\n",
    "\n",
    "print(\"Loaded objects:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266de942-b1e9-4c72-be95-2b0b0744d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean dataset.\n"
     ]
    }
   ],
   "source": [
    "with open(\"synthetic_dataset_clean.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"Saved clean dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77885a3d-4dc0-406a-80a7-91828eddf64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24278aac-8726-44d7-a540-abac04ccc6b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bdf13238a04bc287bad87d84a62dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "     data_files=\"synthetic_dataset_clean.json\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41608691-72c4-4717-af72-e82ab5919443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fefc7f81842466983076b263d3b5d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,  # MI300 â†’ BF16\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8dd7bb-9494-4926-a031-63524fd2add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25,165,824 || all params: 14,795,199,488 || trainable%: 0.1701\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1cb74db-120e-46b7-9a19-92fa82516b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e953a63c943e7b8dfe1682a20095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "dataset = dataset.map(tokenize, remove_columns=[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c40113ee-b545-46d9-b87d-8b7ebf1b8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_lora_output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0fdf8fe-55c0-4341-b81a-b82913d35a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.162100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.5480436038970947, metrics={'train_runtime': 309.3893, 'train_samples_per_second': 5.042, 'train_steps_per_second': 0.323, 'total_flos': 1.3434436989222912e+17, 'train_loss': 0.5480436038970947, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f56266-4cb8-411d-a904-3a24f94b599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen_lora_adapter/tokenizer_config.json',\n",
       " 'qwen_lora_adapter/special_tokens_map.json',\n",
       " 'qwen_lora_adapter/chat_template.jinja',\n",
       " 'qwen_lora_adapter/vocab.json',\n",
       " 'qwen_lora_adapter/merges.txt',\n",
       " 'qwen_lora_adapter/added_tokens.json',\n",
       " 'qwen_lora_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"qwen_lora_adapter\")\n",
    "tokenizer.save_pretrained(\"qwen_lora_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b4cb305-6918-4e33-8d80-c7637cc10d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65270bc08a8f4e3092cf556823ea9222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"A/B/C/D\",\n",
      "  \"reasoning\": \"brief reasoning under 100 words\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"qwen_lora_adapter\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a logical reasoning expert. Answer strictly in valid JSON format with no extra commentary.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Question:\n",
    "Five friends A, B, C, D, E are sitting in a row...\n",
    "Choices:\n",
    "A) A\n",
    "B) B\n",
    "C) C\n",
    "D) D\n",
    "\n",
    "Respond ONLY in this JSON format:\n",
    "{\n",
    "  \"answer\": \"A/B/C/D\",\n",
    "  \"reasoning\": \"brief reasoning under 100 words\"\n",
    "}\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True  # IMPORTANT\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=120,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "import re\n",
    "match = re.search(r'\\{[\\s\\S]*?\\}', decoded)\n",
    "final_output = match.group(0) if match else decoded\n",
    "\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475bafe-0f5d-49e9-b1c1-45589eba76b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
